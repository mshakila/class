{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text feature extraction: representing text as numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "\n",
    "- **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "- **counting** the occurrences of tokens in each document.\n",
    "- **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods we will learn:\n",
    "\n",
    "1. **Bag of words**\n",
    "2. **TF-IDF**\n",
    "3. **Binary representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Bag of Words representation\n",
    "\n",
    "In this scheme, features and samples are defined as follows:\n",
    "- each individual token occurrence frequency (normalized or not) is treated as a feature.\n",
    "- the vector of all the token frequencies for a given document is considered a multivariate sample.\n",
    "\n",
    "This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` implements both tokenization and occurrence counting in a single class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general (not only with bag of words) there are 2 steps to produce a vector representation for each document:\n",
    "   1. Learn the vocubulary in the corpus: this done using the `fit` method.\n",
    "   2. Use that vocabulary to produce the vector representation for each document: this is done using the `transform` method.\n",
    "   \n",
    "Scikit learn provides the `fit_transform` method to perfom the 2 steps at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "          'This is the first document.',\n",
    "          'This is the second second document.',\n",
    "          'And the third one. Yes, yes, yes this',\n",
    "          'Is this the first document?'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step1: learn the 'vocabulary' of the training data (occurs in-place)\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'and',\n",
       " u'document',\n",
       " u'first',\n",
       " u'is',\n",
       " u'one',\n",
       " u'second',\n",
       " u'the',\n",
       " u'third',\n",
       " u'this',\n",
       " u'yes']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary (tokens)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 2: Vectorize the documents (dtm: document-token matrix)\n",
    "X_dtm = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparcity**\n",
    "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).\n",
    "\n",
    "For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the `scipy.sparse` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t2\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 9)\t3\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "# Representation of a sparse matrix\n",
    "print X_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 1, 3],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       and  document  first  is  one  second  the  third  this  yes\n",
       "doc_1    0         1      1   1    0       0    1      0     1    0\n",
       "doc_2    0         1      0   1    0       2    1      0     1    0\n",
       "doc_3    1         0      0   0    1       0    1      1     1    3\n",
       "doc_4    0         1      1   1    0       0    1      0     1    0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of words representation\n",
    "pd.DataFrame(data=X_dtm.toarray(), columns=vectorizer.get_feature_names(), index=['doc_'+str(i+1) for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The converse mapping from feature name to column index is stored in the vocabulary_ attribute of the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_.get('and')\n",
    "print vectorizer.vocabulary_.get('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'and': 0, u'third': 7, u'this': 8, u'is': 3, u'one': 4, u'second': 5, u'the': 6, u'document': 1, u'yes': 9, u'first': 2}\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the vectorizer object has been \"trained\" with the vocabulary from the corpus. Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['this is my second, yes']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the previous corpus, the first and the last documents have exactly the same words hence are encoded in equal vectors. In particular we lose the information that the last document is an interrogative form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x27 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b|,')\n",
    "## The two steps can be done with one call to the fit_transform method\n",
    "X2_dtm = bigram_vectorizer.fit_transform(corpus)\n",
    "X2_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  **  , yes  **  and  **  and the  **  document  **  first  **  first document  **  is  **  is the  **  is this  **  one  **  one yes  **  second  **  second document  **  second second  **  the  **  the first  **  the second  **  the third  **  third  **  third one  **  this  **  this is  **  this the  **  yes  **  yes ,  **  yes this  ** \n"
     ]
    }
   ],
   "source": [
    "for x in bigram_vectorizer.get_feature_names():\n",
    "    print x, ' ** ',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>,</th>\n",
       "      <th>, yes</th>\n",
       "      <th>and</th>\n",
       "      <th>and the</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>first document</th>\n",
       "      <th>is</th>\n",
       "      <th>is the</th>\n",
       "      <th>is this</th>\n",
       "      <th>...</th>\n",
       "      <th>the second</th>\n",
       "      <th>the third</th>\n",
       "      <th>third</th>\n",
       "      <th>third one</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "      <th>this the</th>\n",
       "      <th>yes</th>\n",
       "      <th>yes ,</th>\n",
       "      <th>yes this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ,  , yes  and  and the  document  first  first document  is  is the  \\\n",
       "doc_1  0      0    0        0         1      1               1   1       1   \n",
       "doc_2  0      0    0        0         1      0               0   1       1   \n",
       "doc_3  2      2    1        1         0      0               0   0       0   \n",
       "doc_4  0      0    0        0         1      1               1   1       0   \n",
       "\n",
       "       is this    ...     the second  the third  third  third one  this  \\\n",
       "doc_1        0    ...              0          0      0          0     1   \n",
       "doc_2        0    ...              1          0      0          0     1   \n",
       "doc_3        0    ...              0          1      1          1     1   \n",
       "doc_4        1    ...              0          0      0          0     1   \n",
       "\n",
       "       this is  this the  yes  yes ,  yes this  \n",
       "doc_1        1         0    0      0         0  \n",
       "doc_2        1         0    0      0         0  \n",
       "doc_3        0         0    3      2         1  \n",
       "doc_4        0         1    0      0         0  \n",
       "\n",
       "[4 rows x 27 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=X2_dtm.toarray(), columns=bigram_vectorizer.get_feature_names(), index=['doc_'+str(i+1) for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tf–idf term weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n",
    "\n",
    "**tf–idf** means **term-frequency times inverse document-frequency**.\n",
    "\n",
    "td-idf is intended to reflect how important a word is to a document in a collection or corpus. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it takes a count matrix to a normalized tf or tf-idf representation\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True)\n",
    "tfidf = tfidf_transformer.fit_transform(X_dtm)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453491</td>\n",
       "      <td>0.560151</td>\n",
       "      <td>0.453491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863912</td>\n",
       "      <td>0.225413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225413</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.282339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.282339</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.847017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453491</td>\n",
       "      <td>0.560151</td>\n",
       "      <td>0.453491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            and  document     first        is       one    second       the  \\\n",
       "doc_1  0.000000  0.453491  0.560151  0.453491  0.000000  0.000000  0.370758   \n",
       "doc_2  0.000000  0.275712  0.000000  0.275712  0.000000  0.863912  0.225413   \n",
       "doc_3  0.282339  0.000000  0.000000  0.000000  0.282339  0.000000  0.147336   \n",
       "doc_4  0.000000  0.453491  0.560151  0.453491  0.000000  0.000000  0.370758   \n",
       "\n",
       "          third      this       yes  \n",
       "doc_1  0.000000  0.370758  0.000000  \n",
       "doc_2  0.000000  0.225413  0.000000  \n",
       "doc_3  0.282339  0.147336  0.847017  \n",
       "doc_4  0.000000  0.370758  0.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(data=tfidf.toarray(), columns=vectorizer.get_feature_names(), index=['doc_'+str(i+1) for i in range(4)])\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEoCAYAAACuBsGbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKNj/B5dKDoDwRDagAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8ddaba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_df.loc['doc_2'].plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the tf–idf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the `binary` parameter of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "X_binary = binary_vectorizer.fit_transform(corpus)\n",
    "X_binary.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       and  document  first  is  one  second  the  third  this  yes\n",
       "doc_1    0         1      1   1    0       0    1      0     1    0\n",
       "doc_2    0         1      0   1    0       1    1      0     1    0\n",
       "doc_3    1         0      0   0    1       0    1      1     1    1\n",
       "doc_4    0         1      1   1    0       0    1      0     1    0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary representation\n",
    "pd.DataFrame(data=X_binary.toarray(), columns=binary_vectorizer.get_feature_names(), index=['doc_'+str(i+1) for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
